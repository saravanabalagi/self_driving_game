{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14420, 1, 150, 200)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path_img_src = os.path.join(os.getcwd(), 'data/processed')\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "x = []\n",
    "for file in os.listdir(path_img_src):\n",
    "    ext = os.path.splitext(file)[1]\n",
    "    if ext.lower() not in valid_images: continue\n",
    "    img = cv2.imread(os.path.join(path_img_src,file), cv2.IMREAD_GRAYSCALE)\n",
    "    x.append(img)\n",
    "x = np.array(x)\n",
    "x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Import labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14420, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_labels_src = os.path.join(os.getcwd(), 'data')\n",
    "y = []\n",
    "with open(os.path.join(path_labels_src, 'labels.txt'), \"r\") as labels_file:\n",
    "    for line in labels_file:\n",
    "        y.append(int(line.rstrip('\\n')))\n",
    "y = np.array(y[:x.shape[0]]).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x / 255\n",
    "np.min(x), np.max(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Check label bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 8050, 6: 2000, 5: 1561, 2: 806, 0: 739, 4: 528, 3: 342, 8: 232, 7: 162})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(y.reshape(-1).tolist())\n",
    "print(c)\n",
    "\n",
    "y_orig = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 8050, 6: 2000, 5: 1561, 2: 806, 0: 739, 4: 528, 3: 342, 8: 232, 7: 162})\n",
      "Counter({1: 3679, 6: 2000, 5: 1561, 2: 806, 0: 739, 4: 528, 3: 342, 8: 232, 7: 162})\n"
     ]
    }
   ],
   "source": [
    "y = y_orig\n",
    "\n",
    "if c[6]/c[5] > 1.1:\n",
    "    occ = c[6] - c[5]\n",
    "    idx_6 = np.array(np.where(y.reshape(-1) == 6))[0]\n",
    "    idx = np.random.randint(0, c[6], occ)\n",
    "    idx_6 = idx_6[idx]\n",
    "    np.delete(y,idx_6)\n",
    "    np.delete(x,idx_6)\n",
    "elif c[6]/c[5] < 0.9:\n",
    "    occ = c[6] - c[5]\n",
    "    idx_5 = np.array(np.where(y.reshape(-1) == 5))[0]\n",
    "    idx = np.random.randint(0, c[5], occ)\n",
    "    idx_5 = idx_5[idx]\n",
    "    y = np.delete(y,idx_5)\n",
    "    x = np.delete(x,idx_5, axis=0)\n",
    "print(Counter(y.reshape(-1).tolist()))\n",
    "\n",
    "#check if front is equivalent to right and left\n",
    "if c[1]/(c[6]+c[5]/2) > 1.3:\n",
    "    occ = c[1] - int((c[6]+c[5])/2)\n",
    "    idx_1 = np.array(np.where(y.reshape(-1) == 1))[0]\n",
    "    idx = np.random.randint(0, c[1], occ)\n",
    "    idx_1 = idx_1[idx]\n",
    "    y = np.delete(y,idx_1)\n",
    "    x = np.delete(x,idx_1, axis=0)\n",
    "print(Counter(y.reshape(-1).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Python\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  6\n",
      "After:   [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "number_of_classes = 9\n",
    "print(\"Before: \", y[0])\n",
    "y = to_categorical(y, num_classes=number_of_classes)\n",
    "print(\"After:  \", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Split data for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data | Test Data\n",
      "   8039    |    2010   \n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=np.random)\n",
    "print(\"Train Data | Test Data\")\n",
    "print((\"{0:^10} | {1:^10}\").format(x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Import Keras and use NCHW mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend\n",
    "backend.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Create reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10049, 1, 150, 200) (10049, 9)\n"
     ]
    }
   ],
   "source": [
    "c = x.shape[1]\n",
    "h = x.shape[2]\n",
    "w = x.shape[3]\n",
    "no_of_classes = y.shape[1]\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "def train_model(model, epochs=100, batch_size=125):\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    print(\"\\n\\n\")\n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Accuracy: \", scores[1]*100, \"%\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Create a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8039/8039 [==============================] - 45s 6ms/step - loss: 10.1970 - acc: 0.3647\n",
      "Epoch 2/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 3/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 4/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 5/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 6/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 7/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 8/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 9/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 10/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 11/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 12/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 13/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 14/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 15/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 16/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 17/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 18/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 19/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 20/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 21/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 22/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 23/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 24/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 25/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 26/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 27/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 28/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 29/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 30/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 31/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 32/50\n",
      "8039/8039 [==============================] - 40s 5ms/step - loss: 10.1673 - acc: 0.3692\n",
      "Epoch 33/50\n",
      "5000/8039 [=================>............] - ETA: 15s - loss: 10.2898 - acc: 0.3616"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (7, 7), kernel_initializer='normal', activation='relu', input_shape=(c, h, w)))\n",
    "    model.add(Conv2D(32, (7, 7), kernel_initializer='normal', activation='relu', input_shape=(c, h, w)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(64, (5, 5), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(64, (5, 5), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(64, (5, 5), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(Conv2D(256, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(Conv2D(256, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(Conv2D(256, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # # model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(no_of_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = baseline_model()\n",
    "train_model(model, 50, 125)\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving and loading model architecture and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Define reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "path_models = os.path.join(os.getcwd(), 'models')\n",
    "valid_model_files = [\".h5\", \".json\"]\n",
    "weights_suffix = '_weights.h5'\n",
    "architecture_suffix = '_architecture.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_file_number(path):\n",
    "    numbers = [-1]\n",
    "    for file in os.listdir(path):\n",
    "        filename = os.path.splitext(file)[0]\n",
    "        ext = os.path.splitext(file)[1]\n",
    "        if ext.lower() not in valid_model_files: continue\n",
    "        if filename.startswith('model_'): \n",
    "            numbers.append(int(''.join(list(filter(str.isdigit, filename)))))\n",
    "    counter = max(numbers)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Save current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files:\n",
      "\tmodel_006_architecture.json\n",
      "\tmodel_006_weights.h5\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "if not os.path.isdir(path_models): os.makedirs(path_models)\n",
    "else: counter = get_last_file_number(path_models) + 1\n",
    "\n",
    "model_name = 'model_' + '{0:03d}'.format(counter)\n",
    "model_arch_file = path_models + '\\\\' + model_name + architecture_suffix\n",
    "model_weights_file = path_models + '\\\\' + model_name + weights_suffix\n",
    "with open(model_arch_file, 'w+') as json_file:\n",
    "    json_file.write(model.to_json(indent=4))\n",
    "model.save_weights(model_weights_file)\n",
    "\n",
    "model.save(path_models + '\\\\' + model_name + '.h5')\n",
    "\n",
    "print(\"Saving files:\\n\\t\" + model_name + architecture_suffix + '\\n\\t' + model_name + weights_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files:\n",
      "\tmodel_006_architecture.json\n",
      "\tmodel_006_weights.h5\n"
     ]
    }
   ],
   "source": [
    "counter = get_last_file_number(path_models)\n",
    "    \n",
    "model_name = 'model_' + '{0:03d}'.format(counter)\n",
    "model_arch_file = path_models + '\\\\' + model_name + architecture_suffix\n",
    "model_weights_file = path_models + '\\\\' + model_name + architecture_suffix\n",
    "\n",
    "print(\"Loading files:\\n\\t\" + model_name + architecture_suffix + '\\n\\t' + model_name + weights_suffix)\n",
    "\n",
    "if not os.path.isfile(model_arch_file):\n",
    "    print('Could not find', model_arch_file)\n",
    "elif not os.path.isfile(model_weights_file):\n",
    "    print('Could not find', model_weights_file)\n",
    "else:\n",
    "#     with open(model_arch_file, 'r') as json_file:\n",
    "#         model_loaded = model_from_json(json_file.read())\n",
    "#     model_loaded.load_weights(model_weights_file)\n",
    "    model_loaded = load_model(path_models + '\\\\' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "2003/2003 [==============================] - 1s 506us/step\n",
      "Accuracy:  79.08137794500342 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8992155006511534, 0.7908137794500343]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model_loaded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
