{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24960, 1, 75, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path_img_src = os.path.join(os.getcwd(), 'data/processed')\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "x = []\n",
    "for file in os.listdir(path_img_src):\n",
    "    ext = os.path.splitext(file)[1]\n",
    "    if ext.lower() not in valid_images: continue\n",
    "    img = cv2.imread(os.path.join(path_img_src,file), cv2.IMREAD_GRAYSCALE)\n",
    "    x.append(img)\n",
    "x = np.array(x)\n",
    "x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Import labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24960, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_labels_src = os.path.join(os.getcwd(), 'data')\n",
    "y = []\n",
    "with open(os.path.join(path_labels_src, 'labels.txt'), \"r\") as labels_file:\n",
    "    for line in labels_file:\n",
    "        y.append(int(line.rstrip('\\n')))\n",
    "y = np.array(y[:x.shape[0]]).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x / 255\n",
    "np.min(x), np.max(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Check label bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 16296, 0: 5288, 5: 1239, 6: 1068, 3: 464, 4: 316, 2: 249, 7: 39, 8: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(y.reshape(-1).tolist())\n",
    "print(c)\n",
    "\n",
    "y_orig = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f7c41c89990a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mocc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0midx_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mocc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0midx_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx_5\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mrandint_helpers.pxi\u001b[0m in \u001b[0;36mmtrand._rand_int32\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "y = y_orig\n",
    "\n",
    "if c[6]/c[5] > 1.1:\n",
    "    occ = c[6] - c[5]\n",
    "    idx_6 = np.array(np.where(y.reshape(-1) == 6))[0]\n",
    "    idx = np.random.randint(0, c[6], occ)\n",
    "    idx_6 = idx_6[idx]\n",
    "    np.delete(y,idx_6)\n",
    "    np.delete(x,idx_6)\n",
    "elif c[6]/c[5] < 0.9:\n",
    "    occ = c[6] - c[5]\n",
    "    idx_5 = np.array(np.where(y.reshape(-1) == 5))[0]\n",
    "    idx = np.random.randint(0, c[5], occ)\n",
    "    idx_5 = idx_5[idx]\n",
    "    y = np.delete(y,idx_5)\n",
    "    x = np.delete(x,idx_5, axis=0)\n",
    "print(Counter(y.reshape(-1).tolist()))\n",
    "\n",
    "#check if front is equivalent to right and left\n",
    "if c[1]/(c[6]+c[5]/2) > 1.3:\n",
    "    occ = c[1] - int((c[6]+c[5])/2)\n",
    "    idx_1 = np.array(np.where(y.reshape(-1) == 1))[0]\n",
    "    idx = np.random.randint(0, c[1], occ)\n",
    "    idx_1 = idx_1[idx]\n",
    "    y = np.delete(y,idx_1)\n",
    "    x = np.delete(x,idx_1, axis=0)\n",
    "print(Counter(y.reshape(-1).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  [5]\n",
      "After:   [ 0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "number_of_classes = 9\n",
    "print(\"Before: \", y[0])\n",
    "y = to_categorical(y, num_classes=number_of_classes)\n",
    "print(\"After:  \", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Split data for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data | Test Data\n",
      "  19968    |    4992   \n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=np.random)\n",
    "print(\"Train Data | Test Data\")\n",
    "print((\"{0:^10} | {1:^10}\").format(x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Import Keras and use NCHW mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend\n",
    "backend.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Create reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24960, 1, 75, 100) (24960, 9)\n"
     ]
    }
   ],
   "source": [
    "c = x.shape[1]\n",
    "h = x.shape[2]\n",
    "w = x.shape[3]\n",
    "no_of_classes = y.shape[1]\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "def train_model(model, epochs=100, batch_size=125):\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    print(\"\\n\\n\")\n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Accuracy: \", scores[1]*100, \"%\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Create a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19968/19968 [==============================] - 23s 1ms/step - loss: 1.1530 - acc: 0.6473\n",
      "Epoch 2/50\n",
      "19968/19968 [==============================] - 19s 955us/step - loss: 1.0885 - acc: 0.6515\n",
      "Epoch 3/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 1.0815 - acc: 0.6515\n",
      "Epoch 4/50\n",
      "19968/19968 [==============================] - 19s 957us/step - loss: 1.0797 - acc: 0.6515\n",
      "Epoch 5/50\n",
      "19968/19968 [==============================] - 19s 951us/step - loss: 1.0785 - acc: 0.6515\n",
      "Epoch 6/50\n",
      "19968/19968 [==============================] - 19s 949us/step - loss: 1.0721 - acc: 0.6515\n",
      "Epoch 7/50\n",
      "19968/19968 [==============================] - 19s 958us/step - loss: 1.0605 - acc: 0.6519\n",
      "Epoch 8/50\n",
      "19968/19968 [==============================] - 19s 955us/step - loss: 1.0444 - acc: 0.6558\n",
      "Epoch 9/50\n",
      "19968/19968 [==============================] - 19s 952us/step - loss: 1.0301 - acc: 0.6612\n",
      "Epoch 10/50\n",
      "19968/19968 [==============================] - 19s 957us/step - loss: 1.0023 - acc: 0.6663\n",
      "Epoch 11/50\n",
      "19968/19968 [==============================] - 19s 955us/step - loss: 0.9737 - acc: 0.6777\n",
      "Epoch 12/50\n",
      "19968/19968 [==============================] - 19s 957us/step - loss: 0.9308 - acc: 0.6880\n",
      "Epoch 13/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 0.8901 - acc: 0.7058\n",
      "Epoch 14/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 0.8457 - acc: 0.7239\n",
      "Epoch 15/50\n",
      "19968/19968 [==============================] - 19s 950us/step - loss: 0.8030 - acc: 0.7405\n",
      "Epoch 16/50\n",
      "19968/19968 [==============================] - 19s 955us/step - loss: 0.7652 - acc: 0.7518\n",
      "Epoch 17/50\n",
      "19968/19968 [==============================] - 19s 961us/step - loss: 0.7119 - acc: 0.7702\n",
      "Epoch 18/50\n",
      "19968/19968 [==============================] - 19s 958us/step - loss: 0.6933 - acc: 0.7691\n",
      "Epoch 19/50\n",
      "19968/19968 [==============================] - 19s 956us/step - loss: 0.6657 - acc: 0.7821\n",
      "Epoch 20/50\n",
      "19968/19968 [==============================] - 19s 951us/step - loss: 0.6360 - acc: 0.7904\n",
      "Epoch 21/50\n",
      "19968/19968 [==============================] - 19s 960us/step - loss: 0.6069 - acc: 0.7973\n",
      "Epoch 22/50\n",
      "19968/19968 [==============================] - 19s 965us/step - loss: 0.5790 - acc: 0.8051\n",
      "Epoch 23/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 0.5649 - acc: 0.8087\n",
      "Epoch 24/50\n",
      "19968/19968 [==============================] - 19s 956us/step - loss: 0.5478 - acc: 0.8137\n",
      "Epoch 25/50\n",
      "19968/19968 [==============================] - 19s 956us/step - loss: 0.5228 - acc: 0.8200\n",
      "Epoch 26/50\n",
      "19968/19968 [==============================] - 19s 958us/step - loss: 0.5043 - acc: 0.8229\n",
      "Epoch 27/50\n",
      "19968/19968 [==============================] - 19s 957us/step - loss: 0.4932 - acc: 0.8255\n",
      "Epoch 28/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 0.4837 - acc: 0.8270\n",
      "Epoch 29/50\n",
      "19968/19968 [==============================] - 19s 960us/step - loss: 0.4686 - acc: 0.8321\n",
      "Epoch 30/50\n",
      "19968/19968 [==============================] - 19s 954us/step - loss: 0.4561 - acc: 0.8382\n",
      "Epoch 31/50\n",
      "19968/19968 [==============================] - 19s 958us/step - loss: 0.4445 - acc: 0.8395\n",
      "Epoch 32/50\n",
      "19968/19968 [==============================] - 19s 966us/step - loss: 0.4354 - acc: 0.8415\n",
      "Epoch 33/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 0.4293 - acc: 0.8421\n",
      "Epoch 34/50\n",
      "19968/19968 [==============================] - 19s 953us/step - loss: 0.4207 - acc: 0.8467\n",
      "Epoch 35/50\n",
      "19968/19968 [==============================] - 19s 965us/step - loss: 0.4109 - acc: 0.8489\n",
      "Epoch 36/50\n",
      "19968/19968 [==============================] - 19s 954us/step - loss: 0.4041 - acc: 0.8492\n",
      "Epoch 37/50\n",
      "19968/19968 [==============================] - 19s 955us/step - loss: 0.3981 - acc: 0.8525\n",
      "Epoch 38/50\n",
      "19968/19968 [==============================] - 19s 964us/step - loss: 0.3790 - acc: 0.8574\n",
      "Epoch 39/50\n",
      "19968/19968 [==============================] - 19s 953us/step - loss: 0.3744 - acc: 0.8602\n",
      "Epoch 40/50\n",
      "19968/19968 [==============================] - 19s 953us/step - loss: 0.3693 - acc: 0.8588\n",
      "Epoch 41/50\n",
      "19968/19968 [==============================] - 19s 955us/step - loss: 0.3550 - acc: 0.8652\n",
      "Epoch 42/50\n",
      "19968/19968 [==============================] - 19s 952us/step - loss: 0.3573 - acc: 0.8642\n",
      "Epoch 43/50\n",
      "19968/19968 [==============================] - 19s 957us/step - loss: 0.3529 - acc: 0.8649\n",
      "Epoch 44/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 0.3431 - acc: 0.8687\n",
      "Epoch 45/50\n",
      "19968/19968 [==============================] - 19s 961us/step - loss: 0.3353 - acc: 0.8728\n",
      "Epoch 46/50\n",
      "19968/19968 [==============================] - 19s 960us/step - loss: 0.3401 - acc: 0.8722\n",
      "Epoch 47/50\n",
      "19968/19968 [==============================] - 19s 959us/step - loss: 0.3382 - acc: 0.8697\n",
      "Epoch 48/50\n",
      "19968/19968 [==============================] - 19s 961us/step - loss: 0.3312 - acc: 0.8733\n",
      "Epoch 49/50\n",
      "19968/19968 [==============================] - 19s 958us/step - loss: 0.3217 - acc: 0.8758\n",
      "Epoch 50/50\n",
      "19968/19968 [==============================] - 19s 966us/step - loss: 0.3202 - acc: 0.8769\n",
      "\n",
      "\n",
      "\n",
      "4992/4992 [==============================] - 2s 492us/step\n",
      "Accuracy:  82.4519230769 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.53564166946288871, 0.82451923076923073]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (7, 7), kernel_initializer='normal', activation='relu', input_shape=(c, h, w)))\n",
    "    model.add(Conv2D(32, (7, 7), kernel_initializer='normal', activation='relu', input_shape=(c, h, w)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(64, (5, 5), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(64, (5, 5), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(64, (5, 5), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(Conv2D(256, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(Conv2D(256, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(Conv2D(256, (3, 3), kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # # model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(no_of_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = baseline_model()\n",
    "train_model(model, 50, 125)\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving and loading model architecture and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Define reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "path_models = os.path.join(os.getcwd(), 'models')\n",
    "valid_model_files = [\".h5\", \".json\"]\n",
    "weights_suffix = '_weights.h5'\n",
    "architecture_suffix = '_architecture.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_file_number(path):\n",
    "    numbers = [-1]\n",
    "    for file in os.listdir(path):\n",
    "        filename = os.path.splitext(file)[0]\n",
    "        ext = os.path.splitext(file)[1]\n",
    "        if ext.lower() not in valid_model_files: continue\n",
    "        if filename.startswith('model_'): \n",
    "            numbers.append(int(''.join(list(filter(str.isdigit, filename)))))\n",
    "    counter = max(numbers)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Save current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files:\n",
      "\tmodel_008_architecture.json\n",
      "\tmodel_008_weights.h5\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "if not os.path.isdir(path_models): os.makedirs(path_models)\n",
    "else: counter = get_last_file_number(path_models) + 1\n",
    "\n",
    "model_name = 'model_' + '{0:03d}'.format(counter)\n",
    "model_arch_file = path_models + '\\\\' + model_name + architecture_suffix\n",
    "model_weights_file = path_models + '\\\\' + model_name + weights_suffix\n",
    "with open(model_arch_file, 'w+') as json_file:\n",
    "    json_file.write(model.to_json(indent=4))\n",
    "model.save_weights(model_weights_file)\n",
    "\n",
    "model.save(path_models + '\\\\' + model_name + '.h5')\n",
    "\n",
    "print(\"Saving files:\\n\\t\" + model_name + architecture_suffix + '\\n\\t' + model_name + weights_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files:\n",
      "\tmodel_008_architecture.json\n",
      "\tmodel_008_weights.h5\n"
     ]
    }
   ],
   "source": [
    "counter = get_last_file_number(path_models)\n",
    "    \n",
    "model_name = 'model_' + '{0:03d}'.format(counter)\n",
    "model_arch_file = path_models + '\\\\' + model_name + architecture_suffix\n",
    "model_weights_file = path_models + '\\\\' + model_name + architecture_suffix\n",
    "\n",
    "print(\"Loading files:\\n\\t\" + model_name + architecture_suffix + '\\n\\t' + model_name + weights_suffix)\n",
    "\n",
    "if not os.path.isfile(model_arch_file):\n",
    "    print('Could not find', model_arch_file)\n",
    "elif not os.path.isfile(model_weights_file):\n",
    "    print('Could not find', model_weights_file)\n",
    "else:\n",
    "    # with open(model_arch_file, 'r') as json_file:\n",
    "        # model_loaded = model_from_json(json_file.read())\n",
    "    # model_loaded.load_weights(model_weights_file)\n",
    "    model_loaded = load_model(path_models + '\\\\' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "4992/4992 [==============================] - 2s 495us/step\n",
      "Accuracy:  82.4519230769 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.53564166946288871, 0.82451923076923073]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model_loaded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
